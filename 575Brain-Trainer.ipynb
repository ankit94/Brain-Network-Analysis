{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "#settings\n",
    "pd.options.display.max_columns = 250\n",
    "pd.options.display.max_rows = 250\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn import tree\n",
    "# save the classifier\n",
    "def save_model(model, name):\n",
    "    path = r'{0}.pkl'.format(name)\n",
    "    with open(path, 'wb') as fid:\n",
    "        pickle.dump(model, fid) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate accuracy and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def calculate_accuracy_confusion_matrix(model, test_features, test_labels):\n",
    "    # predicting for test values\n",
    "    DT_test_pred = model.predict(test_features)\n",
    "    clf_DT_accuracy = accuracy_score(test_labels, DT_test_pred, normalize = True)\n",
    "    print(clf_DT_accuracy)\n",
    "\n",
    "    # Confusion Metrices for Decision Tree on train data.\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "#     print(\"Confusion Metrices for Decision Tree\")\n",
    "#     print(\"{0}\".format(metrics.confusion_matrix(test_labels, DT_test_pred, labels=[0, 1])))\n",
    "\n",
    "#     print(\"Classification Report\")\n",
    "#     print(\"{0}\".format(metrics.classification_report(test_labels, DT_test_pred, labels=[0, 1])))\n",
    "    return clf_DT_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>967_x</th>\n",
       "      <th>769_y</th>\n",
       "      <th>517_y</th>\n",
       "      <th>2295_y</th>\n",
       "      <th>2280_x</th>\n",
       "      <th>1970_y</th>\n",
       "      <th>1940_y</th>\n",
       "      <th>1844_y</th>\n",
       "      <th>1838_y</th>\n",
       "      <th>1831_y</th>\n",
       "      <th>1822_y</th>\n",
       "      <th>17_y</th>\n",
       "      <th>13_y</th>\n",
       "      <th>1362_x</th>\n",
       "      <th>1263_y</th>\n",
       "      <th>11_y</th>\n",
       "      <th>978_x</th>\n",
       "      <th>973_x</th>\n",
       "      <th>899_y</th>\n",
       "      <th>826_y</th>\n",
       "      <th>775_y</th>\n",
       "      <th>733_x</th>\n",
       "      <th>705_y</th>\n",
       "      <th>676_x</th>\n",
       "      <th>415_x</th>\n",
       "      <th>340_y</th>\n",
       "      <th>26_x</th>\n",
       "      <th>25_y</th>\n",
       "      <th>2396_x</th>\n",
       "      <th>2372_y</th>\n",
       "      <th>2369_y</th>\n",
       "      <th>2362_y</th>\n",
       "      <th>2360_y</th>\n",
       "      <th>2354_x</th>\n",
       "      <th>2341_y</th>\n",
       "      <th>2319_y</th>\n",
       "      <th>2303_y</th>\n",
       "      <th>2302_y</th>\n",
       "      <th>2286_x</th>\n",
       "      <th>222_x</th>\n",
       "      <th>2168_y</th>\n",
       "      <th>2093_y</th>\n",
       "      <th>2051_y</th>\n",
       "      <th>19_y</th>\n",
       "      <th>1982_y</th>\n",
       "      <th>1975_x</th>\n",
       "      <th>1973_x</th>\n",
       "      <th>192_x</th>\n",
       "      <th>1929_x</th>\n",
       "      <th>1928_x</th>\n",
       "      <th>1846_y</th>\n",
       "      <th>169_y</th>\n",
       "      <th>162_x</th>\n",
       "      <th>161_x</th>\n",
       "      <th>157_y</th>\n",
       "      <th>1560_y</th>\n",
       "      <th>1547_y</th>\n",
       "      <th>1522_y</th>\n",
       "      <th>1519_y</th>\n",
       "      <th>1498_x</th>\n",
       "      <th>1418_y</th>\n",
       "      <th>1417_x</th>\n",
       "      <th>1371_y</th>\n",
       "      <th>1358_x</th>\n",
       "      <th>131_x</th>\n",
       "      <th>1316_x</th>\n",
       "      <th>1224_x</th>\n",
       "      <th>1196_y</th>\n",
       "      <th>1168_x</th>\n",
       "      <th>1083_x</th>\n",
       "      <th>1082_y</th>\n",
       "      <th>1077_x</th>\n",
       "      <th>1044_x</th>\n",
       "      <th>2680</th>\n",
       "      <th>2610</th>\n",
       "      <th>2505</th>\n",
       "      <th>Sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.121856</td>\n",
       "      <td>4845.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3461.0</td>\n",
       "      <td>1.741519</td>\n",
       "      <td>9382.0</td>\n",
       "      <td>3068.0</td>\n",
       "      <td>13928.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>4181.0</td>\n",
       "      <td>20886.0</td>\n",
       "      <td>9176.0</td>\n",
       "      <td>811.0</td>\n",
       "      <td>1.906266</td>\n",
       "      <td>76.0</td>\n",
       "      <td>4448.0</td>\n",
       "      <td>4.216877</td>\n",
       "      <td>2.736703</td>\n",
       "      <td>491.0</td>\n",
       "      <td>1316.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3706.0</td>\n",
       "      <td>2.32688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1085.0</td>\n",
       "      <td>4.674295</td>\n",
       "      <td>2088.0</td>\n",
       "      <td>3.707562</td>\n",
       "      <td>4749.0</td>\n",
       "      <td>749.0</td>\n",
       "      <td>1875.0</td>\n",
       "      <td>11998.0</td>\n",
       "      <td>1.836220</td>\n",
       "      <td>382.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>2320.0</td>\n",
       "      <td>5767.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.584237</td>\n",
       "      <td>3781.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2234.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.832007</td>\n",
       "      <td>3.603820</td>\n",
       "      <td>2.79036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.151265</td>\n",
       "      <td>4604.0</td>\n",
       "      <td>12604.0</td>\n",
       "      <td>1.192857</td>\n",
       "      <td>2.100616</td>\n",
       "      <td>1968.0</td>\n",
       "      <td>18286.0</td>\n",
       "      <td>6691.0</td>\n",
       "      <td>6009.0</td>\n",
       "      <td>6947.0</td>\n",
       "      <td>1.328907</td>\n",
       "      <td>4313.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.916425</td>\n",
       "      <td>3.957857</td>\n",
       "      <td>2.791544</td>\n",
       "      <td>1.318648</td>\n",
       "      <td>8737.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.386674</td>\n",
       "      <td>115.0</td>\n",
       "      <td>2.229106</td>\n",
       "      <td>1.504314</td>\n",
       "      <td>134.310139</td>\n",
       "      <td>0.011348</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2848.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1711.0</td>\n",
       "      <td>2.008449</td>\n",
       "      <td>885.0</td>\n",
       "      <td>2818.0</td>\n",
       "      <td>14600.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3045.0</td>\n",
       "      <td>16278.0</td>\n",
       "      <td>4361.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5184.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1631.0</td>\n",
       "      <td>3589.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>877.0</td>\n",
       "      <td>5.505054</td>\n",
       "      <td>1704.0</td>\n",
       "      <td>6.663014</td>\n",
       "      <td>687.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1446.0</td>\n",
       "      <td>10750.0</td>\n",
       "      <td>2.361541</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>2105.0</td>\n",
       "      <td>6.310055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1898.0</td>\n",
       "      <td>581.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>2184.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.066760</td>\n",
       "      <td>2.255225</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.345670</td>\n",
       "      <td>4513.0</td>\n",
       "      <td>3691.0</td>\n",
       "      <td>1.775757</td>\n",
       "      <td>2.452067</td>\n",
       "      <td>1804.0</td>\n",
       "      <td>7334.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>798.0</td>\n",
       "      <td>635.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1066.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.837766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5135.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.295991</td>\n",
       "      <td>1558.0</td>\n",
       "      <td>6.837252</td>\n",
       "      <td>1.119048</td>\n",
       "      <td>142.058275</td>\n",
       "      <td>0.010882</td>\n",
       "      <td>0.936842</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3553.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2221.0</td>\n",
       "      <td>3.444705</td>\n",
       "      <td>3900.0</td>\n",
       "      <td>1877.0</td>\n",
       "      <td>13104.0</td>\n",
       "      <td>1932.0</td>\n",
       "      <td>4548.0</td>\n",
       "      <td>10556.0</td>\n",
       "      <td>3477.0</td>\n",
       "      <td>657.0</td>\n",
       "      <td>3.302662</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5723.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>107.0</td>\n",
       "      <td>2086.0</td>\n",
       "      <td>4288.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5186.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1814.0</td>\n",
       "      <td>4.015531</td>\n",
       "      <td>3688.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5222.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>3886.0</td>\n",
       "      <td>10244.0</td>\n",
       "      <td>1.974413</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3467.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3190.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>3105.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.935031</td>\n",
       "      <td>2.596250</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.656239</td>\n",
       "      <td>7362.0</td>\n",
       "      <td>4702.0</td>\n",
       "      <td>2.297203</td>\n",
       "      <td>1.629421</td>\n",
       "      <td>3090.0</td>\n",
       "      <td>11089.0</td>\n",
       "      <td>1501.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>723.0</td>\n",
       "      <td>2.040115</td>\n",
       "      <td>2945.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>672.0</td>\n",
       "      <td>4.426983</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8504.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.236922</td>\n",
       "      <td>225.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.461735</td>\n",
       "      <td>203.333911</td>\n",
       "      <td>0.006634</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3745.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1753.0</td>\n",
       "      <td>4576.0</td>\n",
       "      <td>13551.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>5067.0</td>\n",
       "      <td>14872.0</td>\n",
       "      <td>5066.0</td>\n",
       "      <td>3037.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7437.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>150.0</td>\n",
       "      <td>2361.0</td>\n",
       "      <td>4712.0</td>\n",
       "      <td>7.007956</td>\n",
       "      <td>5666.0</td>\n",
       "      <td>3.79940</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1911.0</td>\n",
       "      <td>4.127917</td>\n",
       "      <td>5286.0</td>\n",
       "      <td>6.280764</td>\n",
       "      <td>4792.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>357.0</td>\n",
       "      <td>9802.0</td>\n",
       "      <td>2.496268</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>837.0</td>\n",
       "      <td>2840.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>1217.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2472.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.014550</td>\n",
       "      <td>3.152984</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.404720</td>\n",
       "      <td>2159.0</td>\n",
       "      <td>3441.0</td>\n",
       "      <td>1.992864</td>\n",
       "      <td>1.898039</td>\n",
       "      <td>2402.0</td>\n",
       "      <td>13392.0</td>\n",
       "      <td>573.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2882.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>3.028433</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7323.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.508990</td>\n",
       "      <td>3183.0</td>\n",
       "      <td>5.071188</td>\n",
       "      <td>1.743642</td>\n",
       "      <td>248.175958</td>\n",
       "      <td>0.005993</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.644211</td>\n",
       "      <td>1792.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1863.0</td>\n",
       "      <td>3.523609</td>\n",
       "      <td>4059.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>12089.0</td>\n",
       "      <td>661.0</td>\n",
       "      <td>4176.0</td>\n",
       "      <td>11905.0</td>\n",
       "      <td>2815.0</td>\n",
       "      <td>1925.0</td>\n",
       "      <td>2.946333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6233.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>385.0</td>\n",
       "      <td>2142.0</td>\n",
       "      <td>6394.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2554.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3270.0</td>\n",
       "      <td>5.269713</td>\n",
       "      <td>4392.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>228.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>890.0</td>\n",
       "      <td>7125.0</td>\n",
       "      <td>2.728697</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>511.0</td>\n",
       "      <td>3207.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2359.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>1886.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.373023</td>\n",
       "      <td>5.358855</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.847056</td>\n",
       "      <td>2383.0</td>\n",
       "      <td>4259.0</td>\n",
       "      <td>1.692262</td>\n",
       "      <td>1.852518</td>\n",
       "      <td>2489.0</td>\n",
       "      <td>13499.0</td>\n",
       "      <td>2334.0</td>\n",
       "      <td>929.0</td>\n",
       "      <td>773.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>558.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5593.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.628981</td>\n",
       "      <td>705.0</td>\n",
       "      <td>2.314941</td>\n",
       "      <td>1.034483</td>\n",
       "      <td>207.996222</td>\n",
       "      <td>0.006917</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      967_x   769_y  517_y  2295_y    2280_x  1970_y  1940_y   1844_y  1838_y  \\\n",
       "0  2.121856  4845.0    2.0  3461.0  1.741519  9382.0  3068.0  13928.0   224.0   \n",
       "1  0.000000  2848.0    0.0  1711.0  2.008449   885.0  2818.0  14600.0   193.0   \n",
       "2  0.000000  3553.0    0.0  2221.0  3.444705  3900.0  1877.0  13104.0  1932.0   \n",
       "3  0.000000  3745.0    0.0  1387.0  0.000000  1753.0  4576.0  13551.0   130.0   \n",
       "4  2.644211  1792.0    0.0  1863.0  3.523609  4059.0    18.0  12089.0   661.0   \n",
       "\n",
       "   1831_y   1822_y    17_y    13_y    1362_x  1263_y    11_y     978_x  \\\n",
       "0  4181.0  20886.0  9176.0   811.0  1.906266    76.0  4448.0  4.216877   \n",
       "1  3045.0  16278.0  4361.0   158.0  0.000000    11.0  5184.0  0.000000   \n",
       "2  4548.0  10556.0  3477.0   657.0  3.302662    14.0  5723.0  0.000000   \n",
       "3  5067.0  14872.0  5066.0  3037.0  0.000000     0.0  7437.0  0.000000   \n",
       "4  4176.0  11905.0  2815.0  1925.0  2.946333     0.0  6233.0  0.000000   \n",
       "\n",
       "      973_x  899_y   826_y   775_y     733_x   705_y    676_x  415_x   340_y  \\\n",
       "0  2.736703  491.0  1316.0  3673.0  0.000000  3706.0  2.32688    0.0  1085.0   \n",
       "1  0.000000   15.0  1631.0  3589.0  0.000000  2027.0  0.00000    0.0   877.0   \n",
       "2  0.000000  107.0  2086.0  4288.0  0.000000  5186.0  0.00000    0.0  1814.0   \n",
       "3  0.000000  150.0  2361.0  4712.0  7.007956  5666.0  3.79940    0.0  1911.0   \n",
       "4  0.000000  385.0  2142.0  6394.0  0.000000  2554.0  0.00000    0.0  3270.0   \n",
       "\n",
       "       26_x    25_y    2396_x  2372_y  2369_y  2362_y   2360_y    2354_x  \\\n",
       "0  4.674295  2088.0  3.707562  4749.0   749.0  1875.0  11998.0  1.836220   \n",
       "1  5.505054  1704.0  6.663014   687.0     0.0  1446.0  10750.0  2.361541   \n",
       "2  4.015531  3688.0  0.000000  5222.0   729.0  3886.0  10244.0  1.974413   \n",
       "3  4.127917  5286.0  6.280764  4792.0   213.0   357.0   9802.0  2.496268   \n",
       "4  5.269713  4392.0  0.000000   228.0    79.0   890.0   7125.0  2.728697   \n",
       "\n",
       "   2341_y  2319_y  2303_y  2302_y    2286_x     222_x  2168_y  2093_y  2051_y  \\\n",
       "0   382.0    44.0  2320.0  5767.0  0.000000  4.584237  3781.0   189.0     0.0   \n",
       "1     0.0     4.0   206.0  2105.0  6.310055  0.000000  1898.0   581.0    65.0   \n",
       "2     0.0     4.0    25.0  3467.0  0.000000  0.000000  3190.0   294.0    39.0   \n",
       "3     0.0     0.0   837.0  2840.0  0.000000  0.000000  2140.0  1217.0     0.0   \n",
       "4     2.0     3.0   511.0  3207.0  0.000000  0.000000  2359.0   178.0   434.0   \n",
       "\n",
       "     19_y  1982_y    1975_x    1973_x    192_x  1929_x    1928_x  1846_y  \\\n",
       "0  2234.0     0.0  5.832007  3.603820  2.79036     0.0  3.151265  4604.0   \n",
       "1  2184.0     0.0  7.066760  2.255225  0.00000     0.0  3.345670  4513.0   \n",
       "2  3105.0     2.0  6.935031  2.596250  0.00000     0.0  2.656239  7362.0   \n",
       "3  2472.0     9.0  3.014550  3.152984  0.00000     0.0  2.404720  2159.0   \n",
       "4  1886.0     0.0  3.373023  5.358855  0.00000     0.0  1.847056  2383.0   \n",
       "\n",
       "     169_y     162_x     161_x   157_y   1560_y  1547_y  1522_y  1519_y  \\\n",
       "0  12604.0  1.192857  2.100616  1968.0  18286.0  6691.0  6009.0  6947.0   \n",
       "1   3691.0  1.775757  2.452067  1804.0   7334.0   304.0   798.0   635.0   \n",
       "2   4702.0  2.297203  1.629421  3090.0  11089.0  1501.0   468.0   723.0   \n",
       "3   3441.0  1.992864  1.898039  2402.0  13392.0   573.0   221.0   531.0   \n",
       "4   4259.0  1.692262  1.852518  2489.0  13499.0  2334.0   929.0   773.0   \n",
       "\n",
       "     1498_x  1418_y  1417_x  1371_y    1358_x     131_x    1316_x    1224_x  \\\n",
       "0  1.328907  4313.0     0.0     0.0  3.916425  3.957857  2.791544  1.318648   \n",
       "1  0.000000  1066.0     0.0     0.0  5.837766  0.000000  0.000000  0.000000   \n",
       "2  2.040115  2945.0     0.0   672.0  4.426983  0.000000  0.000000  0.000000   \n",
       "3  0.000000  2882.0     0.0   115.0  3.028433  0.000000  0.000000  0.000000   \n",
       "4  0.000000   558.0     0.0     9.0  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "   1196_y  1168_x    1083_x  1082_y    1077_x    1044_x        2680      2610  \\\n",
       "0  8737.0     0.0  3.386674   115.0  2.229106  1.504314  134.310139  0.011348   \n",
       "1  5135.0     0.0  5.295991  1558.0  6.837252  1.119048  142.058275  0.010882   \n",
       "2  8504.0     0.0  5.236922   225.0  0.000000  2.461735  203.333911  0.006634   \n",
       "3  7323.0     0.0  4.508990  3183.0  5.071188  1.743642  248.175958  0.005993   \n",
       "4  5593.0     0.0  3.628981   705.0  2.314941  1.034483  207.996222  0.006917   \n",
       "\n",
       "       2505  Sex  \n",
       "0  0.947368    1  \n",
       "1  0.936842    0  \n",
       "2  1.000000    1  \n",
       "3  1.000000    1  \n",
       "4  1.000000    1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "df = pd.read_pickle(\"Brain_SelectedData.pkl\")\n",
    "#df = pd.read_pickle(\"BrainDataPCA.pkl\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "       1, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "\n",
    "# target value\n",
    "Y = df2.iloc[:,-1].values\n",
    "# features\n",
    "X = df2.drop(df2.iloc[:,-1].name, axis = 1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.30% in training\n",
      "30.70% in testing\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Splitting the data.\n",
    "split_test_size = 0.3\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(X, Y, test_size =  split_test_size, random_state = 42)\n",
    "\n",
    "# Check whether the correct % data split (70/30 %)Train vs Test data.\n",
    "print(\"{0:0.2f}% in training\".format(  len(train_features)/len(df2.index) *100))\n",
    "print(\"{0:0.2f}% in testing\".format(  len(test_features)/len(df2.index) *100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#KFOLD\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "k=6\n",
    "kf = KFold(n_splits=k, random_state=158, shuffle=True)\n",
    "kf.get_n_splits(X)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3684210526315789\n",
      "0.6842105263157895\n",
      "0.631578947368421\n",
      "0.5789473684210527\n",
      "0.6842105263157895\n",
      "0.8947368421052632\n",
      "KFOLD Avg accuracy0.6403508771929824\n",
      "KFOLD Avg Recall0.6574511137011136\n",
      "KFOLD Avg accuracy0.6575958763458764\n",
      "KFOLD Avg accuracy0.6368158644474434\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix,precision_recall_fscore_support\n",
    "\n",
    "#train the model.\n",
    "sum=0\n",
    "sumRecall=0\n",
    "sumPre=0\n",
    "sumF1=0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "    model = LogisticRegression(C=0.01, solver='liblinear')  #.fit(X_train,y_train)\n",
    "    model.fit(X_train, y_train.ravel())\n",
    "    sum+=calculate_accuracy_confusion_matrix(model, X_test, y_test)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    precision,recall,fbeta_score,support=precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "    \n",
    "    sumRecall+=recall\n",
    "    sumPre+=precision\n",
    "    sumF1+=fbeta_score\n",
    "\n",
    "avg=sum/k\n",
    "print(\"KFOLD Avg accuracy\"+str(avg))\n",
    "avgRecall=sumRecall/k\n",
    "print(\"KFOLD Avg Recall\"+str(avgRecall))\n",
    "avgPre=sumPre/k\n",
    "print(\"KFOLD Avg accuracy\"+str(avgPre))\n",
    "avgF1=sumF1/k\n",
    "print(\"KFOLD Avg accuracy\"+str(avgF1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9714285714285714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9714285714285714"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance on test set\n",
    "calculate_accuracy_confusion_matrix(model, test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "save_model(model, 'LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.631578947368421\n",
      "0.6842105263157895\n",
      "0.631578947368421\n",
      "0.6842105263157895\n",
      "0.7368421052631579\n",
      "0.7368421052631579\n",
      "KFOLD Avg accuracy0.6842105263157895\n",
      "KFOLD Avg Recall0.6813277000777002\n",
      "KFOLD Avg accuracy0.6746182058682059\n",
      "KFOLD Avg accuracy DB0.6842105263157895\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#train the model.\n",
    "sum=0\n",
    "sumRecall=0\n",
    "sumPre=0\n",
    "sumF1=0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "    model = DecisionTreeClassifier(max_depth=6, random_state = 42, criterion='gini') # entropy\n",
    "    model.fit(X_train, y_train)\n",
    "    sum+=calculate_accuracy_confusion_matrix(model, X_test, y_test)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    precision,recall,fbeta_score,support=precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "    \n",
    "    sumRecall+=recall\n",
    "    sumPre+=precision\n",
    "    sumF1+=fbeta_score\n",
    "\n",
    "avg=sum/k\n",
    "print(\"KFOLD Avg accuracy\"+str(avg))\n",
    "avgRecall=sumRecall/k\n",
    "print(\"KFOLD Avg Recall\"+str(avgRecall))\n",
    "avgPre=sumPre/k\n",
    "print(\"KFOLD Avg accuracy\"+str(avgPre))\n",
    "avgF1=sumF1/k\n",
    "avg=sum/k\n",
    "print(\"KFOLD Avg accuracy DB\"+str(avg))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9428571428571428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9428571428571428"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performace on test set\n",
    "calculate_accuracy_confusion_matrix(model, test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "save_model(model, 'DT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7368421052631579\n",
      "0.6842105263157895\n",
      "0.6842105263157895\n",
      "0.6842105263157895\n",
      "0.9473684210526315\n",
      "0.8421052631578947\n",
      "KFOLD Avg accuracy0.763157894736842\n",
      "KFOLD Avg Recall0.7527049339549339\n",
      "KFOLD Avg accuracy0.7689456376956376\n",
      "KFOLD Avg accuracy DB0.763157894736842\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#train the model.\n",
    "sum=0\n",
    "sumRecall=0\n",
    "sumPre=0\n",
    "sumF1=0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "    model = RandomForestClassifier(max_depth=8, n_estimators=25,min_samples_split=10, random_state=0)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    sum+=calculate_accuracy_confusion_matrix(model, X_test, y_test)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    precision,recall,fbeta_score,support=precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "    \n",
    "    sumRecall+=recall\n",
    "    sumPre+=precision\n",
    "    sumF1+=fbeta_score\n",
    "\n",
    "avg=sum/k\n",
    "print(\"KFOLD Avg accuracy\"+str(avg))\n",
    "avgRecall=sumRecall/k\n",
    "print(\"KFOLD Avg Recall\"+str(avgRecall))\n",
    "avgPre=sumPre/k\n",
    "print(\"KFOLD Avg accuracy\"+str(avgPre))\n",
    "avgF1=sumF1/k\n",
    "avg=sum/k\n",
    "print(\"KFOLD Avg accuracy DB\"+str(avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7714285714285715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7714285714285715"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance on test set\n",
    "model.fit(train_features, train_labels)\n",
    "calculate_accuracy_confusion_matrix(model, test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "save_model(model, 'RF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47368421052631576\n",
      "0.5789473684210527\n",
      "0.42105263157894735\n",
      "0.6842105263157895\n",
      "0.5263157894736842\n",
      "0.6842105263157895\n",
      "KFOLD Avg accuracy LightGBM0.5614035087719298\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "#train the model.\n",
    "sum=0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "    model=LGBMClassifier(n_estimators=300, learning_rate=0.01, num_leaves=80, max_depth = 9, colsample_bytree=0.2,\n",
    "            reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40)\n",
    "    model.fit(X_train, y_train)\n",
    "    sum+=calculate_accuracy_confusion_matrix(model, X_test, y_test)\n",
    "\n",
    "avg=sum/k\n",
    "print(\"KFOLD Avg accuracy LightGBM\"+str(avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4857142857142857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4857142857142857"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance of the model on test data\n",
    "calculate_accuracy_confusion_matrix(model, test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "save_model(model, 'LightGBM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6842105263157895\n",
      "0.631578947368421\n",
      "0.631578947368421\n",
      "0.6842105263157895\n",
      "0.8947368421052632\n",
      "0.7368421052631579\n",
      "KFOLD Avg accuracy XGBoost0.7105263157894738\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "sum=0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "    model = xgb.XGBClassifier(booster=\"gbtree\", max_depth=7, objective=\"binary:logistic\", random_state=42, nthread=4, n_estimators = 20, eta = 0.05)\n",
    "    model.fit(X_train, y_train)\n",
    "    sum+=calculate_accuracy_confusion_matrix(model, X_test, y_test)\n",
    "\n",
    "avg=sum/k\n",
    "print(\"KFOLD Avg accuracy XGBoost\"+str(avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9142857142857143\n"
     ]
    }
   ],
   "source": [
    "# performance of the model on test data\n",
    "calculate_accuracy_confusion_matrix(model, test_features, test_labels)\n",
    "# save the model\n",
    "save_model(model, 'XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8421052631578947\n",
      "0.8421052631578947\n",
      "0.7894736842105263\n",
      "0.7368421052631579\n",
      "0.7368421052631579\n",
      "1.0\n",
      "KNN KFOLD Avg accuracy0.8245614035087719\n",
      "KFOLD Avg Recall0.8370062807562807\n",
      "KFOLD Avg Precision0.8419552669552669\n",
      "KFOLD Avg F1 DB0.8203957962539198\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix,precision_recall_fscore_support\n",
    "\n",
    "#train the model.\n",
    "sum=0\n",
    "sumRecall=0\n",
    "sumPre=0\n",
    "sumF1=0\n",
    "for train_index, test_index in kf.split(X):\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40)\n",
    "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    classifier = KNeighborsClassifier(n_neighbors=11)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    sum+=calculate_accuracy_confusion_matrix(classifier, X_test, y_test)\n",
    "    precision,recall,fbeta_score,support=precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "   \n",
    "    sumRecall+=recall\n",
    "    sumPre+=precision\n",
    "    sumF1+=fbeta_score\n",
    "\n",
    "avg=sum/k\n",
    "print(\"KNN KFOLD Avg accuracy\"+str(avg))\n",
    "avgRecall=sumRecall/k\n",
    "print(\"KFOLD Avg Recall\"+str(avgRecall))\n",
    "avgPre=sumPre/k\n",
    "print(\"KFOLD Avg Precision\"+str(avgPre))\n",
    "avgF1=sumF1/k\n",
    "avg=sum/k\n",
    "print(\"KFOLD Avg F1 DB\"+str(avgF1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM:\n",
      "0.5263157894736842\n",
      "0.5789473684210527\n",
      "0.631578947368421\n",
      "0.631578947368421\n",
      "0.5789473684210527\n",
      "0.7368421052631579\n",
      "KFOLD Avg accuracy0.6140350877192983\n",
      "KFOLD Avg Recall0.6133984071484072\n",
      "KFOLD Avg Precision0.6190656565656565\n",
      "KFOLD Avg F1 DB0.5932975228856235\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import svm\n",
    "#SVM\n",
    "print(\"SVM:\")\n",
    "\n",
    "sum=0\n",
    "sumRecall=0\n",
    "sumPre=0\n",
    "sumF1=0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "    clf = svm.SVC(kernel='linear', C=4)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_test_output = clf.predict(X_test)\n",
    "\n",
    "# Compute accuracy based on test samples\n",
    "    sum+=calculate_accuracy_confusion_matrix(clf, X_test, y_test)\n",
    "    precision,recall,fbeta_score,support=precision_recall_fscore_support(y_test, y_test_output, average='macro')\n",
    "    sumRecall+=recall\n",
    "    sumPre+=precision\n",
    "    sumF1+=fbeta_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "avg=sum/k\n",
    "print(\"KFOLD Avg accuracy\"+str(avg))\n",
    "avgRecall=sumRecall/k\n",
    "print(\"KFOLD Avg Recall\"+str(avgRecall))\n",
    "avgPre=sumPre/k\n",
    "print(\"KFOLD Avg Precision\"+str(avgPre))\n",
    "avgF1=sumF1/k\n",
    "avg=sum/k\n",
    "print(\"KFOLD Avg F1 DB\"+str(avgF1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
